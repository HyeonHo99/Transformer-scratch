# Transformer-scratch
### Pytorch Implementation of Transformer Model Presented on ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762.pdf)
<img src="imgs/attention-title.PNG" width="500" height="300"></img>

## The Transformer - Model Architecture
- Encoder-Decoder Structure
- Encoder consists of Encoder Block and Decoder consists of Decoder Blocks (in paper: 6 stacks each) <br>
<img src="imgs/transformer-architecture.PNG" width="400" height="500"></img>

## (1) Embedding - Positional Embedding

## (2) Multi-Head Attention

## (3) Multi-Head Attention

## (4) Encoder Block and Decoder Block

## (5) Encoder

## (6) Decoder


**Mechanism of Multi-Head Attention**

![image](https://user-images.githubusercontent.com/69974410/185332384-fae1ea8f-3f97-4e14-8072-04a19d0176d7.png)

![image](https://user-images.githubusercontent.com/69974410/185332509-f452d2d9-5037-4358-83a9-acfa70357756.png)
